{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db2dae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Install a pip package in the current Jupyter kernel\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7967167",
   "metadata": {},
   "source": [
    "# NOTE: Before executing this ipynb, make sure you have done the following things:\n",
    "### 1. Change the table name of green-db to green-db-backup (from your favorite sql client)\n",
    "\n",
    "```sql\n",
    "ALTER TABLE \"green-db\" RENAME TO \"green-db-backup\";\n",
    "```\n",
    "\n",
    "### 2. Redeploy workers\n",
    "\n",
    "```bash\n",
    "make workers-test-deploy\n",
    "```\n",
    "\n",
    "### 3. Insert old data into new Scraping Tables \n",
    "\n",
    "```sql\n",
    "--DE\n",
    "INSERT INTO \"otto_DE\"\n",
    "\tSELECT \"id\", \"timestamp\", \"merchant\" AS \"source\", \"merchant\", 'DE' AS country, \"category\", \"url\", \"html\", \"page_type\",\n",
    "\t  CASE \n",
    "\t    WHEN meta_information->>'sex' IS NULL THEN 'UNCLASSIFIED'\n",
    "\t\t ELSE upper(meta_information->>'sex')\n",
    "\t\tEND\n",
    "\t\tAS gender,\n",
    "\t  CASE\n",
    "\t    WHEN meta_information\"otto_DE\"->>'sex' IS NULL THEN 'UNCLASSIFIED'\n",
    "\t    ELSE 'ADULT'\n",
    "\t  END\n",
    "\t  AS consumer_lifestage,\n",
    "\t  \"meta_information\"\n",
    "\tFROM otto;\n",
    "\t\n",
    "INSERT INTO \"amazon_DE\"\n",
    "\tSELECT \"id\", \"timestamp\", \"merchant\" AS \"source\", \"merchant\", 'DE' AS country, \"category\", \"url\", \"html\", \"page_type\",\n",
    "\t  CASE \n",
    "\t    WHEN meta_information->>'sex' IS NULL THEN 'UNCLASSIFIED'\n",
    "\t\t ELSE upper(meta_information->>'sex')\n",
    "\t\tEND\n",
    "\t\tAS gender,\n",
    "\t  CASE\n",
    "\t    WHEN meta_information->>'sex' IS NULL THEN 'UNCLASSIFIED'\n",
    "\t    ELSE 'ADULT'\n",
    "\t  END\n",
    "\t  AS consumer_lifestage,\n",
    "\t  \"meta_information\"\n",
    "\tFROM amazon;\n",
    "\t\n",
    "INSERT INTO \"zalando_DE\"\n",
    "\tSELECT \"id\", \"timestamp\", \"merchant\" AS \"source\", \"merchant\", 'DE' AS country, \"category\", \"url\", \"html\", \"page_type\",\n",
    "\t  CASE \n",
    "\t    WHEN meta_information->>'sex' IS NULL THEN 'UNCLASSIFIED'\n",
    "\t\t ELSE upper(meta_information->>'sex')\n",
    "\t\tEND\n",
    "\t\tAS gender,\n",
    "\t  CASE\n",
    "\t    WHEN meta_information->>'sex' IS NULL THEN 'UNCLASSIFIED'\n",
    "\t    ELSE 'ADULT'\n",
    "\t  END\n",
    "\t  AS consumer_lifestage,\n",
    "\t  \"meta_information\"\n",
    "\tFROM zalando;\n",
    "\n",
    "--FR\n",
    "INSERT INTO \"asos_FR\"\n",
    "\tSELECT \"id\", \"timestamp\", \"merchant\" AS \"source\", \"merchant\", 'FR' AS country, \"category\", \"url\", \"html\", \"page_type\",\n",
    "\t  CASE \n",
    "\t    WHEN meta_information->>'sex' IS NULL THEN 'UNCLASSIFIED'\n",
    "\t\t ELSE upper(meta_information->>'sex')\n",
    "\t\tEND\n",
    "\t\tAS gender,\n",
    "\t  CASE\n",
    "\t    WHEN meta_information->>'sex' IS NULL THEN 'UNCLASSIFIED'\n",
    "\t    ELSE 'ADULT'\n",
    "\t  END\n",
    "\t  AS consumer_lifestage,\n",
    "\t  \"meta_information\"\n",
    "\tFROM asos;\n",
    "\n",
    "INSERT INTO \"hm_FR\"\n",
    "\tSELECT \"id\", \"timestamp\", \"merchant\" AS \"source\", \"merchant\", 'FR' AS country, \"category\", \"url\", \"html\", \"page_type\",\n",
    "\t  CASE \n",
    "\t    WHEN meta_information->>'sex' IS NULL THEN 'UNCLASSIFIED'\n",
    "\t\t ELSE upper(meta_information->>'sex')\n",
    "\t\tEND\n",
    "\t\tAS gender,\n",
    "\t  CASE\n",
    "\t    WHEN meta_information->>'sex' IS NULL THEN 'UNCLASSIFIED'\n",
    "\t    ELSE 'ADULT'\n",
    "\t  END\n",
    "\t  AS consumer_lifestage,\n",
    "\t  \"meta_information\"\n",
    "\tFROM hm;\n",
    "\n",
    "INSERT INTO \"zalando_FR\"\n",
    "\tSELECT \"id\", \"timestamp\", 'zalando' AS \"source\", 'zalando' AS \"merchant\", 'FR' AS country, \"category\", \"url\", \"html\", \"page_type\",\n",
    "\t  CASE \n",
    "\t    WHEN meta_information->>'sex' IS NULL THEN 'UNCLASSIFIED'\n",
    "\t\t ELSE upper(meta_information->>'sex')\n",
    "\t\tEND\n",
    "\t\tAS gender,\n",
    "\t  CASE\n",
    "\t    WHEN meta_information->>'sex' IS NULL THEN 'UNCLASSIFIED'\n",
    "\t    ELSE 'ADULT'\n",
    "\t  END\n",
    "\t  AS consumer_lifestage,\n",
    "\t  \"meta_information\"\n",
    "\tFROM zalando_fr;\n",
    "\n",
    "INSERT INTO \"amazon_FR\"\n",
    "\tSELECT \"id\", \"timestamp\", 'amazon' AS \"source\", 'amazon' AS \"merchant\", 'FR' AS country, \"category\", \"url\", \"html\", \"page_type\",\n",
    "\t  CASE \n",
    "\t    WHEN meta_information->>'sex' IS NULL THEN 'UNCLASSIFIED'\n",
    "\t\t ELSE upper(meta_information->>'sex')\n",
    "\t\tEND\n",
    "\t\tAS gender,\n",
    "\t  CASE\n",
    "\t    WHEN meta_information->>'sex' IS NULL THEN 'UNCLASSIFIED'\n",
    "\t    ELSE 'ADULT'\n",
    "\t  END\n",
    "\t  AS consumer_lifestage,\n",
    "\t  \"meta_information\"\n",
    "\tFROM amazon_fr;\n",
    "\n",
    "\n",
    "--GB\n",
    "INSERT INTO \"zalando_GB\"\n",
    "\tSELECT \"id\", \"timestamp\", 'zalando' AS \"source\", 'zalando' AS \"merchant\", 'GB' AS country, \"category\", \"url\", \"html\", \"page_type\",\n",
    "\t  CASE \n",
    "\t    WHEN meta_information->>'sex' IS NULL THEN 'UNCLASSIFIED'\n",
    "\t\t ELSE upper(meta_information->>'sex')\n",
    "\t\tEND\n",
    "\t\tAS gender,\n",
    "\t  CASE\n",
    "\t    WHEN meta_information->>'sex' IS NULL THEN 'UNCLASSIFIED'\n",
    "\t    ELSE 'ADULT'\n",
    "\t  END\n",
    "\t  AS consumer_lifestage,\n",
    "\t  \"meta_information\"\n",
    "\tFROM zalando_uk;\n",
    "```\n",
    "\n",
    "### 4. Set autoincrements for all scraping tables\n",
    "\n",
    "```sql\n",
    "-- SET autoincrements\n",
    "Select setval('\"zalando_DE_id_seq\"'::REGCLASS, (select max(id) FROM \"zalando_DE\"));\n",
    "Select setval('\"zalando_FR_id_seq\"'::REGCLASS, (select max(id) FROM \"zalando_FR\"));\n",
    "Select setval('\"zalando_GB_id_seq\"'::REGCLASS, (select max(id) FROM \"zalando_GB\"));\n",
    "\n",
    "Select setval('\"hm_FR_id_seq\"'::REGCLASS, (select max(id) FROM \"hm_FR\"));\n",
    "Select setval('\"asos_FR_id_seq\"'::REGCLASS, (select max(id) FROM \"asos_FR\"));\n",
    "Select setval('\"otto_DE_id_seq\"'::REGCLASS, (select max(id) FROM \"otto_DE\"));\n",
    "\n",
    "Select setval('\"amazon_FR_id_seq\"'::REGCLASS, (select max(id) FROM \"amazon_FR\"));\n",
    "Select setval('\"amazon_DE_id_seq\"'::REGCLASS, (select max(id) FROM \"amazon_DE\"));\n",
    "```\n",
    "\n",
    "### 5. Add postgres passwords in the following cell\n",
    "\n",
    "### 6. Run this ipynb\n",
    "\n",
    "**6.1 load scraping tables**\n",
    "\n",
    "**6.2 load old greendb table** \n",
    "\n",
    "**6.3 merge scraping and greendb (except asos)**\n",
    "\n",
    "**6.4 join asos exclusively**\n",
    "\n",
    "**6.5 combine joined asos and all others**\n",
    "\n",
    "**6.6 insert merged data into new greendb**\n",
    "\n",
    "### 7. Set autoincrements for green-db table\n",
    "\n",
    "```sql\n",
    "Select setval('\"green-db_id_seq\"'::REGCLASS, (select max(id) FROM \"green-db\"));\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42716a5",
   "metadata": {},
   "source": [
    "# 6. RUN this ipynb\n",
    "## 6.1 load scraping tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c10dea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"POSTGRES_SCRAPING_USER\"] = 'scraping'\n",
    "os.environ[\"POSTGRES_SCRAPING_PASSWORD\"] = '' # TODO\n",
    "os.environ[\"POSTGRES_SCRAPING_HOST\"] = '127.0.0.1'\n",
    "os.environ[\"POSTGRES_SCRAPING_PORT\"] = '5432'\n",
    "\n",
    "os.environ[\"POSTGRES_GREEN_DB_USER\"] = \"green-db\"\n",
    "os.environ[\"POSTGRES_GREEN_DB_PASSWORD\"] = \"\" # TODO\n",
    "os.environ[\"POSTGRES_GREEN_DB_HOST\"] = \"localhost\"\n",
    "os.environ[\"POSTGRES_GREEN_DB_PORT\"] = \"5432\"\n",
    "\n",
    "from database.connection import Scraping\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1abf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import ARRAY, BIGINT, INTEGER, JSON, NUMERIC, TEXT, TIMESTAMP, VARCHAR, Column\n",
    "\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "from typing import List, Optional\n",
    "\n",
    "from pydantic import BaseModel, conint, conlist\n",
    "\n",
    "\n",
    "class PageType(str, Enum):\n",
    "    SERP = \"SERP\"\n",
    "    PRODUCT = \"PRODUCT\"\n",
    "\n",
    "\n",
    "class ScrapedPageNoHTML(BaseModel):\n",
    "    timestamp: datetime\n",
    "    source: str\n",
    "    merchant: str\n",
    "    country: str\n",
    "    url: str\n",
    "    #page_type: PageType\n",
    "    category: str\n",
    "    gender: str\n",
    "    consumer_lifestage: str\n",
    "    #meta_information: dict\n",
    "\n",
    "    class Config:\n",
    "        orm_mode = True\n",
    "        use_enum_values = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd1cd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from database.connection import Scraping, Connection\n",
    "from core.domain import ScrapedPage\n",
    "from message_queue import MessageQueue\n",
    "from typing import Iterator\n",
    "\n",
    "\n",
    "class Scraping2(Scraping):      \n",
    "    def get_scraped_products_no_html(self, batch_size: int = 1000) -> Iterator[ScrapedPage]:\n",
    "        \"\"\"\n",
    "        Fetch all `ScrapedPage`s.\n",
    "\n",
    "        Args:        \n",
    "            batch_size (int, optional): How many rows to fetch simultaneously. Defaults to 1000.\n",
    "\n",
    "        Yields:\n",
    "            Iterator[ScrapedPage]: Iterator over the domain object representations\n",
    "        \"\"\"\n",
    "        with self._session_factory() as db_session:\n",
    "            query = db_session.query(self._database_class.id, \n",
    "                                     self._database_class.timestamp, \n",
    "                                     self._database_class.source,\n",
    "                                     self._database_class.merchant, \n",
    "                                     self._database_class.country, \n",
    "                                     self._database_class.url, \n",
    "                                     #self._database_class.page_type, \n",
    "                                     self._database_class.category, \n",
    "                                     self._database_class.gender, \n",
    "                                     self._database_class.consumer_lifestage,\n",
    "                                     #self._database_class.meta_information,\n",
    "                                    ).filter(\n",
    "                self._database_class.page_type == 'PRODUCT'\n",
    "            )\n",
    "            return (ScrapedPageNoHTML.from_orm(row) for row in query.all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51f877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_2_db = {\n",
    "    \"zalando_DE\": Scraping2(\"zalando_DE\"),\n",
    "    \"zalando_GB\": Scraping2(\"zalando_GB\"),\n",
    "    \"zalando_FR\": Scraping2(\"zalando_FR\"),\n",
    "    \"otto_DE\": Scraping2(\"otto_DE\"),      \n",
    "    \"hm_FR\": Scraping2(\"hm_FR\"),\n",
    "    \"amazon_DE\": Scraping2(\"amazon_DE\"),\n",
    "    \"amazon_FR\": Scraping2(\"amazon_FR\"),\n",
    "    \"asos_FR\": Scraping2(\"asos_FR\"),\n",
    "}\n",
    "\n",
    "# name_2_db = {\n",
    "#     \"zalando\": Scraping2(\"zalando\"),\n",
    "#     \"zalando_uk\": Scraping2(\"zalando_uk\"),\n",
    "#     \"zalando_fr\": Scraping2(\"zalando_fr\"),\n",
    "#     \"otto\": Scraping2(\"otto\"),      \n",
    "#     \"hm\": Scraping2(\"hm\"),\n",
    "#     \"amazon\": Scraping2(\"amazon\"),\n",
    "#     \"amazon_fr\": Scraping2(\"amazon_fr\"),\n",
    "#     \"asos\": Scraping2(\"asos\"),\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e97e57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_scraping_products = pd.DataFrame()\n",
    "\n",
    "for name, db in name_2_db.items():\n",
    "    products_iterator = db.get_scraped_products_no_html()\n",
    "    products = pd.DataFrame([product.__dict__ for product in products_iterator])\n",
    "    \n",
    "    all_scraping_products = pd.concat([all_scraping_products, products])\n",
    "    \n",
    "all_scraping_products.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1d1462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if kernel dies it might be good to have stored the df locally\n",
    "#all_products.to_parquet(\"../data/products-meta-2022-06-13.parquet\", index=False) #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238f8565",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scraping_products.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1548e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scraping_products[\"merchant\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937f5d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scraping_products.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fb608e",
   "metadata": {},
   "source": [
    "## 6.2 load old greendb table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff246eab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from database.tables import GreenDBBaseTable, __TableMixin\n",
    "from sqlalchemy import ARRAY, BIGINT, INTEGER, JSON, NUMERIC, TEXT, TIMESTAMP, VARCHAR, Column\n",
    "\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "from typing import List, Optional\n",
    "\n",
    "from pydantic import BaseModel, conint, conlist\n",
    "\n",
    "from core.sustainability_labels import create_CertificateType\n",
    "\n",
    "CertificateType = create_CertificateType()\n",
    "\n",
    "\n",
    "class GreenDBTableOld(GreenDBBaseTable, __TableMixin):\n",
    "    \"\"\"\n",
    "    Defines the GreenDB columns.\n",
    "\n",
    "    Args:\n",
    "        GreenDBBaseTable ([type]): `sqlalchemy` base class for the GreenDB database\n",
    "        __TableMixin ([type]): Mixin that implements some convenience methods\n",
    "    \"\"\"\n",
    "\n",
    "    __tablename__ = \"green-db-backup\" #TODO\n",
    "\n",
    "    id = Column(INTEGER, nullable=False, autoincrement=True, primary_key=True)\n",
    "    timestamp = Column(TIMESTAMP, nullable=False)\n",
    "    merchant = Column(TEXT, nullable=False)\n",
    "    category = Column(TEXT, nullable=False)\n",
    "    url = Column(TEXT, nullable=False)\n",
    "    name = Column(TEXT, nullable=False)\n",
    "    description = Column(TEXT, nullable=False)\n",
    "    brand = Column(TEXT, nullable=False)\n",
    "    sustainability_labels = Column(ARRAY(TEXT), nullable=False)  # TODO foreign keys to labels\n",
    "    price = Column(NUMERIC, nullable=False)\n",
    "    currency = Column(TEXT, nullable=False)\n",
    "    image_urls = Column(ARRAY(TEXT), nullable=False)\n",
    "\n",
    "    color = Column(TEXT, nullable=True)\n",
    "    size = Column(TEXT, nullable=True)\n",
    "\n",
    "    gtin = Column(BIGINT, nullable=True)\n",
    "    asin = Column(TEXT, nullable=True)\n",
    "    \n",
    "class CurrencyType(str, Enum):\n",
    "    EUR = \"EUR\"\n",
    "    GBP = \"GBP\"\n",
    "    \n",
    "class Productold(BaseModel):\n",
    "    timestamp: datetime\n",
    "    url: str\n",
    "    merchant: str\n",
    "    category: str\n",
    "    name: str\n",
    "    description: str\n",
    "    brand: str\n",
    "    sustainability_labels: conlist(CertificateType, min_items=1)  # type: ignore\n",
    "    price: float\n",
    "    currency: CurrencyType\n",
    "    image_urls: List[str]\n",
    "\n",
    "    color: Optional[str]\n",
    "    size: Optional[str]\n",
    "\n",
    "    # int, source: https://support.google.com/merchants/answer/6219078?hl=en\n",
    "    gtin: Optional[int]\n",
    "\n",
    "    # str because alpha numeric\n",
    "    # source: https://en.wikipedia.org/wiki/Amazon_Standard_Identification_Number\n",
    "    asin: Optional[str]\n",
    "\n",
    "    class Config:\n",
    "        orm_mode = True\n",
    "        use_enum_values = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc58f901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from database.connection import Connection\n",
    "from core.constants import DATABASE_NAME_GREEN_DB\n",
    "from database.tables import SustainabilityLabelsTable\n",
    "from typing import Iterator\n",
    "from core.domain import Product\n",
    "\n",
    "class GreenDBold(Connection):\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        `Connection` for the GreenDB.\n",
    "        Automatically pre-populates the sustainability labels table.\n",
    "        \"\"\"\n",
    "        super().__init__(GreenDBTableOld, DATABASE_NAME_GREEN_DB)\n",
    "    \n",
    "    def get_all_products(self, batch_size: int = 10000) -> Iterator[Product]:\n",
    "        \"\"\"\n",
    "        Fetch all `Products`.\n",
    "\n",
    "        Args:            \n",
    "            batch_size (int, optional): How many rows to fetch simultaneously. Defaults to 1000.\n",
    "\n",
    "        Yields:\n",
    "            Iterator[ScrapedPage]: Iterator over the domain object representations\n",
    "        \"\"\"\n",
    "        with self._session_factory() as db_session:\n",
    "            query = db_session.query(self._database_class)\n",
    "            return (Productold.from_orm(row) for row in query.all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776e8619",
   "metadata": {},
   "outputs": [],
   "source": [
    "greenDBold = GreenDBold()\n",
    "\n",
    "products_iterator = greenDBold.get_all_products()\n",
    "greenDBproducts = pd.DataFrame([product.__dict__ for product in products_iterator])\n",
    "greenDBproducts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb34144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "greenDBproducts.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004d825f",
   "metadata": {},
   "source": [
    "\n",
    "## 6.3 merge scraping and greendb (except asos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c65fb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#store asos seperately\n",
    "scraping_asos = all_scraping_products[all_scraping_products[\"merchant\"]==\"asos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc89d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep index for joining\n",
    "\n",
    "tuples = list(zip(all_scraping_products['timestamp'], all_scraping_products['url'], all_scraping_products['category']))\n",
    "index = pd.MultiIndex.from_tuples(tuples, names=[\"timestamp\", \"url\", \"category\"])\n",
    "\n",
    "all_scraping_products.index = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253d15d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove unwanted columns\n",
    "all_scraping_products = all_scraping_products[[\"source\", \"country\", \"gender\", \"consumer_lifestage\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55741c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scraping_products.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c455569",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "joined = greenDBproducts.join(all_scraping_products, on=['timestamp', 'url', 'category'])\n",
    "joined.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c27b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates due to joining\n",
    "\n",
    "# the shops might list products in multiple categories which we have just assigned to one category\n",
    "# this creates duplicate extracted products\n",
    "# so joining the df, using just timestamp, url and catgeory (green-db specific category) \n",
    "# we can not fully map instancs from scraping table to green-db table\n",
    "# but for including gender information this is not necessary.\n",
    "# If we have multiple products with same url and different categories assigned to different genders\n",
    "# the extracted products are just duplicates of each other, \n",
    "# because the product is basically a UNISEX product assigned to multiple categories\n",
    "\n",
    "joined_deduplicated = joined.drop_duplicates(subset=['timestamp', 'url', 'source', 'merchant', 'category', 'gender', 'consumer_lifestage'])\n",
    "joined_deduplicated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e72f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_deduplicated.merchant.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109e8ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check null values. Only color, size, gtin and asin should have null values!\n",
    "# null values in source, country gender and consumer_lifestage are due to asos\n",
    "joined_deduplicated.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8b2d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# amount of None encapsulated as string (will be transformed later)\n",
    "joined_deduplicated[joined_deduplicated[\"size\"]==\"None\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbab11c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove asos\n",
    "joined_deduplicated = joined_deduplicated[joined_deduplicated[\"merchant\"]!=\"asos\"]\n",
    "joined_deduplicated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd39274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only color, size, gtin, asin should be null\n",
    "joined_deduplicated.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc0816c",
   "metadata": {},
   "source": [
    "## 6.4 join asos exclusively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dfa844",
   "metadata": {},
   "outputs": [],
   "source": [
    "greendb_asos = greenDBproducts[greenDBproducts[\"merchant\"]==\"asos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eb22aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraping_asos[\"product_id\"] = scraping_asos[\"url\"].apply(lambda x: x.split(\"/\")[-1].split(\"?\")[0])\n",
    "scraping_asos[\"product_id\"] = scraping_asos[\"product_id\"].astype('int64')\n",
    "\n",
    "greendb_asos[\"product_id\"] = greendb_asos[\"url\"].apply(lambda x: x.split(\"/\")[-1])\n",
    "greendb_asos[\"product_id\"] = greendb_asos[\"product_id\"].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffdf6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep product index\n",
    "tuples = list(zip( greendb_asos['product_id'], greendb_asos['timestamp'], greendb_asos['category']))\n",
    "index = pd.MultiIndex.from_tuples(tuples, names=[\"timestamp\", \"url\", \"category\"])\n",
    "#greendb_asos[\"id\"] = greendb.index\n",
    "greendb_asos.index = index\n",
    "\n",
    "#prep index for joining\n",
    "tuples = list(zip(scraping_asos['product_id'],scraping_asos['timestamp'], scraping_asos['category']))\n",
    "index = pd.MultiIndex.from_tuples(tuples, names=[\"timestamp\", \"url\", \"category\"])\n",
    "#scraping[\"id\"] = scraping.index\n",
    "scraping_asos.index = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9863dd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove unwanted columns\n",
    "scraping_asos = scraping_asos[[\"source\", \"country\", \"gender\", \"consumer_lifestage\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adddc57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "asos_joined = greendb_asos.join(scraping_asos, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4003f86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "asos_joined.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24da5dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicates\n",
    "asos_joined_deduplicated = asos_joined.drop_duplicates(subset=['timestamp', 'url', 'merchant', 'category', 'gender'])\n",
    "asos_joined_deduplicated.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cbf526",
   "metadata": {},
   "source": [
    "## 6.5 combine joined asos and all others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16069903",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_deduplicated = pd.concat([asos_joined_deduplicated, joined_deduplicated])\n",
    "joined_deduplicated.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1890d331",
   "metadata": {},
   "source": [
    "## 6.6 insert merged data into new greendb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0f7ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from database.connection import GreenDB\n",
    "from core.domain import Product\n",
    "\n",
    "class GreenDBnew(GreenDB):\n",
    "    def write_df(self, df):\n",
    "        \"\"\"\n",
    "        Writes a `dataframe` into the database.        \n",
    "        \"\"\"\n",
    "        with self._session_factory() as db_session:\n",
    "            i = 0\n",
    "            df_len = len(df)\n",
    "            for index, product in df.iterrows():\n",
    "                i += 1\n",
    "                try:                    \n",
    "                    db_object = self._database_class(**Product.parse_obj(product).dict())\n",
    "                    db_session.add(db_object)\n",
    "                except Exception as e:\n",
    "                    print(f\"error for product with index: {index}\")\n",
    "                    print(e)\n",
    "                #commit every 1000 products and at the end\n",
    "                if (i % 1000 == 0) or (i == df_len):\n",
    "                    db_session.commit()\n",
    "                    print(f\"Commited {i} products\")\n",
    "\n",
    "\n",
    "greenDBnew = GreenDBnew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2feb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform merchant\n",
    "merchant_to_new_merchant = {\n",
    "    \"asos\": \"asos\",\n",
    "    \"amazon\": \"amazon\",\n",
    "    \"amazon_fr\": \"amazon\",\n",
    "    \"zalando\": \"zalando\",\n",
    "    \"zalando_fr\": \"zalando\",\n",
    "    \"zalando_uk\": \"zalando\",\n",
    "    \"otto\": \"otto\",\n",
    "    \"hm\": \"hm\"\n",
    "}\n",
    "\n",
    "joined_deduplicated[\"merchant\"] = joined_deduplicated[\"merchant\"].apply(lambda x: merchant_to_new_merchant.get(x))\n",
    "\n",
    "# transform color\n",
    "joined_deduplicated[\"colors\"] = joined_deduplicated[\"color\"].apply(lambda x: [x] if x else None)\n",
    "\n",
    "# transform size\n",
    "joined_deduplicated[\"sizes\"] = joined_deduplicated[\"size\"].apply(lambda sizes: None if sizes == 'None' or sizes is None else sizes.split(\", \"))\n",
    "\n",
    "# need to convert NA to None, otherwise pydantic throws errors\n",
    "joined_deduplicated = joined_deduplicated.replace({np.nan: None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567f9e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test pydantic product parsing\n",
    "Product.parse_obj(joined_deduplicated.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2caaab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "greenDBnew.write_df(joined_deduplicated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb970e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataframe size is: {len(joined_deduplicated)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3831070e",
   "metadata": {},
   "source": [
    "Check product count in database and compare with dataframe size\n",
    "```sql\n",
    "Select Count(id) from \"green-db\";\n",
    "```\n",
    "\n",
    "If everything went well and the tables look good, you can delete the old tables e.g. via:\n",
    "\n",
    "```sql\n",
    "DROP TABLE \"amazon\", \"asos\", \"otto\", \"zalando\", \"zalando_fr\", \"zalando_uk\";\n",
    "DROP TABLE \"green-db-backup\";\n",
    "```\n",
    "\n",
    "Do not forget to set the autoincrement for green-db (see step 7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poetry-extract",
   "language": "python",
   "name": "poetry-extract"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
